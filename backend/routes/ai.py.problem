"""
AI聊天与思维导图API路由
实现AI问答、流式聊天和思维导图生成功能
"""

import json
import logging
from flask import Blueprint, request, jsonify
from typing import Dict, List, Optional

from services.zhipu_client import get_zhipu_client
from middleware.auth import jwt_required_custom, get_current_user_id

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create blueprint
ai_bp = Blueprint('ai', __name__, url_prefix='/api/ai')


@ai_bp.route('/chat', methods=['POST'])
@jwt_required_custom
def chat():
    """
    AI问答（非流式）

    Request:
        {
            "question": "什么是Transformer?",
            "paper_id": "2301.00001",  // 可选，关联特定论文
            "chat_history": [  // 可选，对话历史
                {"role": "user", "content": "..."},
                {"role": "assistant", "content": "..."}
            ],
            "api_config": {
                "model": "glm-4-flash",  // 可选，默认模型
                "temperature": 0.7,       // 可选
                "max_tokens": 2000         // 可选
            }
        }

    Response:
        {
            "success": true,
            "data": {
                "answer": "Transformer是一种...",
                "model": "glm-4-flash",
                "usage": {
                    "prompt_tokens": 100,
                    "completion_tokens": 500,
                    "total_tokens": 600
                }
            }
        }
    """
    try:
        data = request.get_json()
        question = data.get('question', '').strip()
        paper_id = data.get('paper_id')
        chat_history = data.get('chat_history', [])
        api_config = data.get('api_config', {})

        # 验证问题
        if not question:
            return jsonify({
                'success': False,
                'error': '问题不能为空'
            }), 400

        # 获取API客户端
        api_key = api_config.get('api_key')
        if api_key:
            from services.zhipu_client import ZhipuClient
            client = ZhipuClient(api_key=api_key)
        else:
            client = get_zhipu_client()

        # 构建消息历史
        messages = []

        # 添加系统提示
        system_prompt = "你是一个专业的学术论文助手，擅长解释复杂的学术概念和研究成果。"
        messages.append({"role": "system", "content": system_prompt})

        # 添加当前问题
        messages.append({"role": "user", "content": question})

        # 获取模型参数
        model = api_config.get('model', 'glm-4-flash')
        temperature = api_config.get('temperature', 0.7)
        max_tokens = api_config.get('max_tokens', 2000)

        # 调用AI
        logger.info(f"发送AI聊天请求，模型: {model}")
        result = client.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=False
        )

        if not result.get('success'):
            return jsonify({
                'success': False,
                'error': result.get('error', 'AI请求失败')
            }), 500

        # 提取回答
        response_data = result.get('data', {})
        choices = response_data.get('choices', [])

        if not choices:
            return jsonify({
                'success': False,
                'error': 'AI未返回有效响应'
            }), 500

        answer = choices[0].get('message', {}).get('content', '')
        usage = response_data.get('usage', {})

        logger.info(f"AI聊天成功，生成 {usage.get('completion_tokens', 0)} tokens")

        return jsonify({
            'success': True,
            'data': {
                'answer': answer,
                'model': response_data.get('model', model),
                'usage': {
                    'prompt_tokens': usage.get('prompt_tokens', 0),
                    'completion_tokens': usage.get('completion_tokens', 0),
                    'total_tokens': usage.get('total_tokens', 0)
                }
            }
        })

    except Exception as e:
        logger.error(f"AI聊天错误: {e}")
        return jsonify({
            'success': False,
            'error': f'处理请求时发生错误: {str(e)}'
        }), 500


@ai_bp.route('/chat/stream', methods=['POST'])
@jwt_required_custom
def chat_stream():
    """
    AI问答（流式SSE）

    Request: 同 /api/ai/chat

    Response: text/event-stream (SSE流式输出)
        data: {"content": "Trans"}
        data: {"content": "former"}
        data: {"content": " is..."}
        data: [DONE]
    """
    try:
        data = request.get_json()
        question = data.get('question', '').strip()
        paper_id = data.get('paper_id')
        chat_history = data.get('chat_history', [])
        api_config = data.get('api_config', {})

        # 验证问题
        if not question:
            return jsonify({
                'success': False,
                'error': '问题不能为空'
            }), 400

        # 获取API客户端
        api_key = api_config.get('api_key')
        if api_key:
            from services.zhipu_client import ZhipuClient
            client = ZhipuClient(api_key=api_key)
        else:
            client = get_zhipu_client()

        # 构建消息历史
        messages = []

        # 添加系统提示
        system_prompt = "你是一个专业的学术论文助手，擅长解释复杂的学术概念和研究成果。"
        messages.append({"role": "system", "content": system_prompt})

        # 添加当前问题
        messages.append({"role": "user", "content": question})

        # 获取模型参数
        model = api_config.get('model', 'glm-4-flash')
        temperature = api_config.get('temperature', 0.7)
        max_tokens = api_config.get('max_tokens', 2000)

        logger.info(f"发送流式AI聊天请求，模型: {model}")

        def generate():
            """生成SSE流"""
            try:
                # 使用流式API
                stream = client.chat_completion_stream(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens
                )

                # 处理流式响应
                for chunk in stream:
                    # 格式化为SSE
                    sse_data = json.dumps({"content": chunk}, ensure_ascii=False)
                    yield f"data: {sse_data}\n\n"

                # 发送结束标记
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"流式生成错误: {e}")
                error_data = json.dumps({"error": str(e)}, ensure_ascii=False)
                yield f"data: {error_data}\n\n"
                yield "data: [DONE]\n\n"

        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        logger.error(f"流式聊天初始化错误: {e}")
        return jsonify({
            'success': False,
            'error': f'处理请求时发生错误: {str(e)}'
        }), 500
